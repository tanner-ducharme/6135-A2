{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdN5pnC8MMx-"
      },
      "source": [
        "This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and encoder_decoder_solution.py\n",
        "\n",
        "This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Amazon Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFHMMDtSwuW4"
      },
      "outputs": [],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oODLwt1QzgGa"
      },
      "outputs": [],
      "source": [
        "#@title Link your assignment folder & install requirements\n",
        "#@markdown Enter the path to the assignment folder in your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. \n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "folder = \"\" #@param {type:\"string\"}\n",
        "!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
        "\n",
        "# Add the assignment folder to Python path\n",
        "if '/content/assignment' not in sys.path:\n",
        "  sys.path.insert(0, '/content/assignment')\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  warnings.warn('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > ParamÃ¨tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import urllib.request\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import List, Dict, Union, Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "!pip install -qqq datasets transformers --upgrade\n",
        "import transformers\n",
        "from transformers import AutoModel\n",
        "from datasets import Dataset, load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "from encoder_decoder_solution import EncoderDecoder\n",
        "from transformer_solution import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGJeUTOyPEHR"
      },
      "outputs": [],
      "source": [
        "dataset_train = load_dataset(\"amazon_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n",
        "dataset_test = load_dataset(\"amazon_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn4myFpBOmQ2"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ” Quick look at the data { run: \"auto\" }\n",
        "#@markdown Lets have quick look at a few samples in our test set.\n",
        "n_samples_to_see = 3 #@param {type: \"integer\"}\n",
        "for i in range(n_samples_to_see):\n",
        "  print(\"-\"*30)\n",
        "  print(\"title:\", dataset_test[i][\"title\"])\n",
        "  print(\"content:\", dataset_test[i][\"content\"])\n",
        "  print(\"label:\", dataset_test[i][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxwRDQFUtaG"
      },
      "source": [
        "### 1ï¸âƒ£ Tokenize the `text`\n",
        "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
        "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
        "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
        "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
        "number of wordpieces when segmented according to the chosen wordpiece model.\n",
        "\n",
        "Under this model:\n",
        "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
        "2. Some words will be mapped to multiple tokens!\n",
        "3. Depending on the kind of model, your tokens may or may not respect capitalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCpNwaTYSo3U"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OMDqabyToBt"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ” Quick look at tokenization { run: \"auto\", vertical-output: true }\n",
        "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n",
        "tokenizer.tokenize(input_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEu6aqReXqp6"
      },
      "source": [
        "### 2ï¸âƒ£ Encoding\n",
        "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpDGccrvYKnT"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ” Quick look at token encoding { run: \"auto\"}\n",
        "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n",
        "\n",
        "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
        "print(\"-.\"*15)\n",
        "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI8lFKZSZ2ZW"
      },
      "source": [
        "### 3ï¸âƒ£ Truncate/Pad samples\n",
        "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SP31MsbHZxp5"
      },
      "outputs": [],
      "source": [
        "class Collate:\n",
        "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
        "        self.tokenizer_name = tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
        "        texts = list(map(lambda batch_instance: batch_instance[\"title\"], batch))\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "        \n",
        "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
        "        labels = torch.LongTensor(labels)\n",
        "        return dict(tokenized_inputs, **{\"labels\": labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4VaSpuyIjNqn"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ§‘â€ðŸ³ Setting up the collate function { run: \"auto\" }\n",
        "tokenizer_name = \"bert-base-uncased\" #@param {type: \"string\"}\n",
        "sample_max_length = 256 #@param {type:\"slider\", min:32, max:512, step:1}\n",
        "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y9P4oWyOSexA"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone_hidden_size = backbone_hidden_size\n",
        "        self.nb_classes = nb_classes\n",
        "        self.back_bone = AutoModel.from_pretrained(\n",
        "            self.backbone,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = back_bone_output[0]\n",
        "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "class ReviewClassifierLSTM(nn.Module):\n",
        "    def __init__(self, nb_classes: int, encoder_only: bool = False, dropout=0.5):\n",
        "        super(ReviewClassifierLSTM, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.encoder_only = encoder_only\n",
        "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only) \n",
        "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
        "       \n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        hidden_states, _ = self.back_bone(input_ids, attention_mask)\n",
        "        pooled_output = hidden_states \n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "class ReviewClassifierTransformer(nn.Module):\n",
        "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n",
        "        super(ReviewClassifierTransformer, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
        "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
        "        hidden_states = back_bone_output\n",
        "        pooled_output = hidden_states\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_one_epoch(\n",
        "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    epoch_loss = 0\n",
        "    logging_loss = 0\n",
        "    start_time = time.time()\n",
        "    mini_start_time = time.time()\n",
        "    for step, batch in enumerate(training_data_loader):\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        logging_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % logging_frequency == 0:\n",
        "            freq_time = time.time()-mini_start_time\n",
        "            logger['train_time'].append(freq_time+logger['train_time'][-1])\n",
        "            logger['train_losses'].append(logging_loss/logging_frequency)\n",
        "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
        "            eval_acc, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
        "            logger['eval_accs'].append(eval_acc)\n",
        "            logger['eval_losses'].append(eval_loss)\n",
        "            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n",
        "\n",
        "            logging_loss = 0\n",
        "            mini_start_time = time.time()\n",
        "\n",
        "    return epoch_loss / len(training_data_loader), time.time()-start_time\n",
        "\n",
        "\n",
        "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    eval_loss = 0\n",
        "    correct_predictions = {i: 0 for i in range(2)}\n",
        "    total_predictions = {i: 0 for i in range(2)}\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(test_data_loader):\n",
        "            batch = {key: value.to(device) for key, value in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs[0]\n",
        "            eval_loss += loss.item()\n",
        "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
        "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
        "                if target == prediction:\n",
        "                    correct_predictions[target] += 1\n",
        "                total_predictions[target] += 1\n",
        "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
        "    model.train()\n",
        "    return accuracy,  eval_loss / len(test_data_loader), time.time() - start_time\n"
      ],
      "metadata": {
        "id": "y7YNfMAuotk2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logger(model):\n",
        "  logger = dict()\n",
        "  logger['train_time'] = [0]\n",
        "  logger['eval_time'] = [0]\n",
        "  logger['train_losses'] = []\n",
        "  logger['eval_accs'] = []\n",
        "  logger['eval_losses'] = []\n",
        "  logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n",
        "  return logger\n",
        "\n",
        "def put_in_dictionary(train_loss, train_time, eval_loss, eval_time, eval_acc):\n",
        "  logger[\"total_train_loss\"] = train_loss\n",
        "  logger[\"total_train_time\"] = train_time\n",
        "  logger[\"final_eval_loss\"] = eval_loss\n",
        "  logger[\"final_eval_time\"] = eval_time\n",
        "  logger[\"final_eval_acc\"] = eval_acc\n",
        "  logger['train_time'] = logger['train_time'][1:]\n",
        "  logger['eval_time'] = logger['eval_time'][1:]\n",
        "\n",
        "def save_logs(dictionary, log_dir, exp_id):\n",
        "  log_dir = os.path.join(log_dir, exp_id)\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  # Log arguments\n",
        "  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
        "    json.dump(dictionary, f, indent=2)\n"
      ],
      "metadata": {
        "id": "6ELvN-rLk67b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP58LrWUjFt4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"--> Device selected: {device}\")\n",
        "\n",
        "nb_epoch = 1\n",
        "batch_size = 512\n",
        "logging_frequency = 5 \n",
        "learning_rate = 1e-5\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "for i in range(1, 7):\n",
        "  experimental_setting = i\n",
        "  # 4 experimental settings\n",
        "  torch.random.manual_seed(0)\n",
        "\n",
        "  if experimental_setting == 1:\n",
        "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
        "  if experimental_setting == 2:\n",
        "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False)\n",
        "  if experimental_setting == 3:\n",
        "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
        "  if experimental_setting == 4:\n",
        "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
        "  if experimental_setting == 5:\n",
        "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
        "  if experimental_setting == 6: \n",
        "    model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
        "    for parameter in model.back_bone.parameters():\n",
        "      parameter.requires_grad= False\n",
        "    logging_frequency = 703\n",
        "  # setting up the optimizer\n",
        "  optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
        "  model.to(device)\n",
        "  logger = get_logger(model)\n",
        "  train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
        "  eval_acc, eval_loss, eval_time  = evaluate(model, test_loader)\n",
        "  put_in_dictionary(train_loss, train_time, eval_loss, eval_time, eval_acc)\n",
        "  print(f\"    Epoch: {1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
        "  save_logs(logger, \"assignment/log\", str(experimental_setting))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
